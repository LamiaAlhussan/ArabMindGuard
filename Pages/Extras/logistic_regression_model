# logistic_regression_model.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
from camel_tools.tokenizers.morphological import MorphologicalTokenizer
from camel_tools.disambig.mle import MLEDisambiguator
from camel_tools.tagger.default import DefaultTagger
from camel_tools.utils.normalize import normalize_unicode
from camel_tools.utils.dediac import dediac_ar
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import string
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from camel_tools.tokenizers.morphological import MorphologicalTokenizer
from camel_tools.disambig.mle import MLEDisambiguator
from camel_tools.tagger.default import DefaultTagger
from camel_tools.utils.normalize import normalize_unicode
from camel_tools.utils.dediac import dediac_ar
import demoji
import re
import demoji
from nltk.stem.isri import ISRIStemmer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import demoji
from sklearn.svm import SVC


# import subprocess

# # To install all datasets
# # subprocess.run(['camel_data', '-i', 'all'], check=True)

# # or just the datasets for morphology and MLE disambiguation only
# subprocess.run(['camel_data', '-i', 'light'], check=True)

# # or just the default datasets for each component
# subprocess.run(['camel_data', '-i', 'defaults'], check=True)
# Load your dataset
stemmer = ISRIStemmer()
mle = MLEDisambiguator.pretrained()
#nltk.download('punkt')

dataset_path = 'rr.csv'
df = pd.read_csv(dataset_path)

def preprocess(tweet):
    normal_sen = tweet.split()
    d1tok_tokenizer = MorphologicalTokenizer(disambiguator=mle, scheme='d1tok', split='True')
    Tok_sen = d1tok_tokenizer.tokenize(normal_sen)
    tagger = DefaultTagger(mle, 'pos')
    tags = tagger.tag(Tok_sen)
    sentence = ''

    for i in range(len(Tok_sen)):
        if tags[i] == "noun" or tags[i] == "adj" or tags[i] == "verb":
            sentence += " " + Tok_sen[i]

    Tok_sen = d1tok_tokenizer.tokenize(sentence.split())
    disambiguated = mle.disambiguate(Tok_sen)
    stem_word = ' '.join([d.analyses[0].analysis['lex'] for d in disambiguated])
    sentence = normalize_unicode(stem_word)
    sentence = dediac_ar(sentence)
    sentence = processDocument(sentence, stemmer)

    return sentence


#-------------
def remove_hashtag(df, col = 'Tweets'):
    for letter in r'#.][!XR':
      df[col] = df[col].astype(str).str.replace(letter,'', regex=True)

remove_hashtag(df)

#-------------
arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations
# nltk.download('stopwords')

def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)

    return text.translate(translator)


def remove_stopwords(text):
    arabic_pronouns = ["انا", "انت", "هو", "هي", "نحن", "انتم", "هم", "هن",'الي','اني','ان',"علي",'الا','لان','لكن','او',"كنت","كان"]
    pattern = re.compile(r'\bو(\w+)\b', flags=re.UNICODE)

    # Use sub() to replace matches with an empty string
    cleaned_text = pattern.sub(r'\1', text)
    # Your existing preprocessing steps
    stop_words = set(stopwords.words("arabic"))

    # Tokenization

    # Remove stop words
    word_tokens = word_tokenize(text)

    text = ' '.join([word for word in word_tokens if word.strip() not in stop_words and word.strip() not in arabic_pronouns])

    return text

#-------------
def normalize_arabic(text):
    text = re.sub("[إأآٱا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)
    return text
#-------------
def remove_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)
#-------------
def processDocument(doc, stemmer):

    #Replace @username with empty string
    doc = re.sub(r'@[^\s]+', ' ', doc)
    doc = re.sub(r'_', ' ', doc)
    doc = re.sub(r'\n', ' ', doc)
    doc = re.sub(r'[a-z,A-Z]', '', doc)
    doc = re.sub(r'\d', '', doc)
    #Convert www.* or https?://* to " "
    doc = re.sub('((www\.[^\s]+)|(https?://[^\s]+))',' ',doc)
    #Replace #word with word
    doc = re.sub(r'#([^\s]+)', r'\1', doc)
    # remove punctuations
    doc= remove_punctuations(doc)
    # normalize the tweet
    doc= normalize_arabic(doc)
    # remove repeated letters
    doc=remove_repeating_char(doc)
    # Remove emojis
    doc = demoji.replace(doc, repl="")

    # Remove remaining non-alphanumeric characters
    doc = re.sub(r'\W', ' ', doc)
   # doc = remove_stopwords(doc)

    # Remove extra whitespaces
    doc = re.sub(r'\s+', ' ', doc).strip()
    #doc = ' '.join([word for word in doc.split() if len(word.strip())>2])


    #stemming
   # doc = stemmer.stem(doc)


    return doc

df["Tweets"] = df['Tweets'].apply(lambda x: preprocess(x))

X = df['Tweets']
y = df['Label']

combined_tweets = [(tweet, label) for tweet, label in zip(X, y)]

# Separate depressed and non-depressed tweets
depressed_tweets = [(tweet, label) for tweet, label in combined_tweets if label == 1]
non_depressed_tweets = [(tweet, label) for tweet, label in combined_tweets if label == 2]



# Split the data into training and testing sets with an equal number of depressed and non-depressed tweets
X_train_depressed, X_test_depressed, y_train_depressed, y_test_depressed = train_test_split(
    [tweet for tweet, _ in depressed_tweets], [label for _, label in depressed_tweets],
    test_size=0.2, random_state=42, stratify=[label for _, label in depressed_tweets]
)

X_train_non_depressed, X_test_non_depressed, y_train_non_depressed, y_test_non_depressed = train_test_split(
    [tweet for tweet, _ in non_depressed_tweets], [label for _, label in non_depressed_tweets],
    test_size=0.2, random_state=42, stratify=[label for _, label in non_depressed_tweets]
)
# Combine the depressed and non-depressed training sets
X_train = X_train_depressed + X_train_non_depressed
y_train = y_train_depressed + y_train_non_depressed

# Combine the depressed and non-depressed testing sets
X_test = X_test_depressed + X_test_non_depressed
y_test = y_test_depressed + y_test_non_depressed

# X_train, X_test, y_train, y_test = train_test_split(df['Tweets'], df['Label'], test_size=0.4, random_state=42)

vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,3))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Fine-tune Logistic Regression parameters using GridSearchCV
param_grid = {'C': [0.00001,0.0001, 0.001, 0.01, 0.1, 1, 10,100]}
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train_tfidf, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters: 1,3", best_params)

# Use the best parameters to train the model
best_model = grid_search.best_estimator_
best_model.fit(X_train_tfidf, y_train)

# predictions on the test set
y_pred = best_model.predict(X_test_tfidf)


# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
# Save the trained model and vectorizer
# Save the trained model and vectorizer
print(accuracy*100)

print('-------------------------')
# Example with Support Vector Machine (SVM)
svm_model = SVC(kernel='linear', C=1.0, random_state=42)
svm_model.fit(X_train_tfidf, y_train)
y_pred_svm = svm_model.predict(X_test_tfidf)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"SVM Accuracy: {accuracy_svm:.2f}")

#--
svm_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
                  'kernel': ['linear', 'rbf', 'poly'],
                  'gamma': ['scale', 'auto']}

svm_grid_search = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=5, scoring='accuracy')
svm_grid_search.fit(X_train_tfidf, y_train)

# Get the best parameters for SVM
best_svm_params = svm_grid_search.best_params_
print("Best SVM Parameters:", best_svm_params)

# Use the best parameters to train the SVM model
best_svm_model = svm_grid_search.best_estimator_
best_svm_model.fit(X_train_tfidf, y_train)

# predictions on the test set for SVM
y_pred_svm_tuned = best_svm_model.predict(X_test_tfidf)

# Evaluate the tuned SVM model
accuracy_svm_tuned = accuracy_score(y_test, y_pred_svm_tuned)
print(f"Tuned SVM Accuracy: {accuracy_svm_tuned:.2f}")
print('Tuned SVM train accuracy', best_svm_model.score(X_train_tfidf, y_train))
print('Tuned SVM test accuracy', best_svm_model.score(X_test_tfidf, y_test))
# print('Tuned SVM cross-validation', cross_val_score(best_svm_model, X_train_tfidf, y_train).mean())
# Display other metrics for the tuned SVM model
# print(classification_report(y_test, y_pred_svm_tuned))

joblib.dump(accuracy, 'logistic_regression_model.pkl')
# joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

