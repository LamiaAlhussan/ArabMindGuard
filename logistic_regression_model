# logistic_regression_model.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
from camel_tools.tokenizers.morphological import MorphologicalTokenizer
from camel_tools.disambig.mle import MLEDisambiguator
from camel_tools.tagger.default import DefaultTagger
from camel_tools.utils.normalize import normalize_unicode
from camel_tools.utils.dediac import dediac_ar
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import string
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from camel_tools.tokenizers.morphological import MorphologicalTokenizer
from camel_tools.disambig.mle import MLEDisambiguator
from camel_tools.tagger.default import DefaultTagger
from camel_tools.utils.normalize import normalize_unicode
from camel_tools.utils.dediac import dediac_ar
import demoji
import re
import demoji
from nltk.stem.isri import ISRIStemmer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import demoji

# import subprocess

# # To install all datasets
# # subprocess.run(['camel_data', '-i', 'all'], check=True)

# # or just the datasets for morphology and MLE disambiguation only
# subprocess.run(['camel_data', '-i', 'light'], check=True)

# # or just the default datasets for each component
# subprocess.run(['camel_data', '-i', 'defaults'], check=True)
# Load your dataset
stemmer = ISRIStemmer()
mle = MLEDisambiguator.pretrained()
nltk.download('punkt')

dataset_path = 'rr.csv'
df = pd.read_csv(dataset_path)

def preprocess(tweet):
    normal_sen = tweet.split()
    d1tok_tokenizer = MorphologicalTokenizer(disambiguator=mle, scheme='d1tok', split='True')
    Tok_sen = d1tok_tokenizer.tokenize(normal_sen)
    tagger = DefaultTagger(mle, 'pos')
    tags = tagger.tag(Tok_sen)
    sentence = ''

    for i in range(len(Tok_sen)):
        if tags[i] == "noun" or tags[i] == "adj" or tags[i] == "verb":
            sentence += " " + Tok_sen[i]

    Tok_sen = d1tok_tokenizer.tokenize(sentence.split())
    disambiguated = mle.disambiguate(Tok_sen)
    stem_word = ' '.join([d.analyses[0].analysis['lex'] for d in disambiguated])
    sentence = normalize_unicode(stem_word)
    sentence = dediac_ar(sentence)
    sentence = processDocument(sentence, stemmer)

    return sentence


#-------------
def remove_hashtag(df, col = 'Tweets'):
    for letter in r'#.][!XR':
      df[col] = df[col].astype(str).str.replace(letter,'', regex=True)

remove_hashtag(df)

#-------------
arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations
nltk.download('stopwords')

def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)

    return text.translate(translator)


def remove_stopwords(text):
    arabic_pronouns = ["انا", "انت", "هو", "هي", "نحن", "انتم", "هم", "هن",'الي','اني','ان',"علي",'الا','لان','لكن','او',"كنت","كان"]
    pattern = re.compile(r'\bو(\w+)\b', flags=re.UNICODE)

    # Use sub() to replace matches with an empty string
    cleaned_text = pattern.sub(r'\1', text)
    # Your existing preprocessing steps
    stop_words = set(stopwords.words("arabic"))

    # Tokenization

    # Remove stop words
    word_tokens = word_tokenize(text)

    text = ' '.join([word for word in word_tokens if word.strip() not in stop_words and word.strip() not in arabic_pronouns])

    return text

#-------------
def normalize_arabic(text):
    text = re.sub("[إأآٱا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)
    return text
#-------------
def remove_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)
#-------------
def processDocument(doc, stemmer):

    #Replace @username with empty string
    doc = re.sub(r'@[^\s]+', ' ', doc)
    doc = re.sub(r'_', ' ', doc)
    doc = re.sub(r'\n', ' ', doc)
    doc = re.sub(r'[a-z,A-Z]', '', doc)
    doc = re.sub(r'\d', '', doc)
    #Convert www.* or https?://* to " "
    doc = re.sub('((www\.[^\s]+)|(https?://[^\s]+))',' ',doc)
    #Replace #word with word
    doc = re.sub(r'#([^\s]+)', r'\1', doc)
    # remove punctuations
    doc= remove_punctuations(doc)
    # normalize the tweet
    doc= normalize_arabic(doc)
    # remove repeated letters
    doc=remove_repeating_char(doc)
    # Remove emojis
    doc = demoji.replace(doc, repl="")

    # Remove remaining non-alphanumeric characters
    doc = re.sub(r'\W', ' ', doc)
    doc = remove_stopwords(doc)

    # Remove extra whitespaces
    doc = re.sub(r'\s+', ' ', doc).strip()
    doc = ' '.join([word for word in doc.split() if len(word.strip())>2])


    #stemming
    doc = stemmer.stem(doc)


    return doc

df["Tweets"] = df['Tweets'].apply(lambda x: preprocess(x))

X = df['Tweets']
y = df['Label']
X_train, X_test, y_train, y_test = train_test_split(df['Tweets'], df['Label'], test_size=0.4, random_state=42)

vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Fine-tune Logistic Regression parameters using GridSearchCV
param_grid = {'C': [0.00001,0.0001, 0.001, 0.01, 0.1, 1, 10]}
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train_tfidf, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Use the best parameters to train the model
best_model = grid_search.best_estimator_
best_model.fit(X_train_tfidf, y_train)

# predictions on the test set
y_pred = best_model.predict(X_test_tfidf)


# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
# Save the trained model and vectorizer
# Save the trained model and vectorizer
print(accuracy*100)
joblib.dump(accuracy, 'logistic_regression_model.pkl')
# joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

